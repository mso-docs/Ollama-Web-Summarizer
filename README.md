# Ollama Web Summarizer Chrome Extension

![Ollama Summarizer Logo](icons/Ollama-Summarizer.png)

A Chrome extension that uses your local Ollama to summarize web content, explain concepts, and chat about it.

## Project Structure

```
ollama-ext/
â”œâ”€â”€ manifest.json          # Extension manifest
â”œâ”€â”€ README.md              # Main documentation
â”œâ”€â”€ src/                   # Source code
â”‚   â”œâ”€â”€ background.js      # Service worker for API calls
â”‚   â”œâ”€â”€ content.js         # Content script for page interaction
â”‚   â”œâ”€â”€ popup.html         # Extension popup UI
â”‚   â”œâ”€â”€ popup.css          # Popup styles with theme support
â”‚   â””â”€â”€ popup.js           # Popup logic and theme system
â”œâ”€â”€ icons/                 # Image assets
â”‚   â”œâ”€â”€ background-llama.png
â”‚   â”œâ”€â”€ llama-chat.png
â”‚   â”œâ”€â”€ icon128.svg
â”‚   â””â”€â”€ Ollama-Summarizer.png
â”œâ”€â”€ scripts/               # Helper scripts
â”‚   â”œâ”€â”€ start-ollama.bat   # Windows Ollama startup
â”‚   â”œâ”€â”€ start-ollama.sh    # Unix Ollama startup
â”‚   â””â”€â”€ create-icons.sh    # Icon generation script
â””â”€â”€ docs/                  # Documentation
    â””â”€â”€ INSTALLATION.md    # Detailed setup guide
```

## Features

- ðŸ“„ **Summarize any web page** - Click the extension to get instant summaries
- ðŸ’¬ **Interactive chat** - Ask follow-up questions about the content
- âœ¨ **Text selection popup** - Highlight text to summarize, explain, or ask questions
- ðŸ–±ï¸ **Right-click context menu** - Quick access to AI features on any text
- âš¡ **Real-time streaming** - See responses as they're generated with "I'm thinking..." indicators
- ðŸ”„ **Auto-detect models** - Automatically loads all your installed Ollama models into the dropdown
- ðŸŽ¨ **12 themes** - Customize appearance with light/dark color schemes
- ðŸ”’ **100% local** - All processing happens on your machine via Ollama
- ðŸš€ **No external API calls** - Complete privacy

## Prerequisites

- [Ollama](https://ollama.ai) installed and running locally
- At least one model installed:
  ```bash
  ollama pull llama3.2
  # Or any other model: mistral, qwen2.5, deepseek-r1, etc.
  ```

## Quick Install

### 1. Set CORS environment variable (one-time)
**Windows:** `[System.Environment]::SetEnvironmentVariable('OLLAMA_ORIGINS', '*', 'User')`  
**Mac/Linux:** `echo 'export OLLAMA_ORIGINS="*"' >> ~/.bashrc && source ~/.bashrc`

### 2. Install a model (if you haven't already)
```bash
ollama pull llama3.2
```

### 3. Start Ollama
```bash
ollama serve
```

### 4. Load extension
1. Chrome â†’ `chrome://extensions/` â†’ Enable "Developer mode"
2. Click "Load unpacked" â†’ Select `ollama-ext` folder

**ðŸ“– For detailed setup instructions, see [INSTALLATION.md](docs/INSTALLATION.md)**

## Usage

1. Navigate to any webpage
2. Click the extension icon in your Chrome toolbar
3. Click "Summarize Page" to get a summary
4. Use the chat below to ask follow-up questions about the content

## Configuration

### Model Selection
The extension automatically detects all models installed in your Ollama instance by calling the `/api/tags` endpoint. The model dropdown will populate with your available models - no manual configuration needed!

**How it works:**
- On startup, the extension queries `http://localhost:11434/api/tags`
- All installed models (same as `ollama list`) appear in the dropdown
- Select any model to use it for summarization and chat
- Your selection is saved for future sessions

**To add more models:**
```bash
ollama pull llama3.2
ollama pull mistral
ollama pull deepseek-r1
```

Reload the extension and the new models will appear in the dropdown.

### Themes
Choose from 12 color themes (6 light, 6 dark) in the theme dropdown. Your preference is saved automatically.

## Quick Troubleshooting

| Error | Solution |
|-------|----------|
| **403 Forbidden** | Set `OLLAMA_ORIGINS="*"` environment variable (see install step 1) |
| **404 Model not found** | Run `ollama pull <model-name>` or select different model |
| **Connection Error** | Start Ollama with `ollama serve` |
| **Empty model dropdown** | Ensure Ollama is running and has models (`ollama list`) |

**ðŸ“– For complete troubleshooting guide, see [INSTALLATION.md](docs/INSTALLATION.md)**
